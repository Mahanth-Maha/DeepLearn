# Optimizers

Notebook : [Optimizers](./All_gradient_descents.ipynb)

Experimented on different optimization Techniques and compare. implementing the paper **An overview of gradient descent optimization algorithms**  [Sebastian Ruder, 2016, arXiv 1609.04747](http://arxiv.org/abs/1609.04747).

## Optimizers

- **Gradient Descent**
- **Stochastic Gradient Descent (SGD)**
- **Momentum based Gradient Descent**
- **Nesterov Accelerated Gradient (NAG)**
- **Adagrad**
- **Adadelta**
- **RMSprop**
- **Adam**
- **AdaMax**
- **Nadam**
- **AdamW** 
